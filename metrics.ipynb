{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d21c0203",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a7b373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4825\n",
      "747\n"
     ]
    }
   ],
   "source": [
    "cols = ['label', 'text']\n",
    "df = pd.read_csv(r'dataset/spam.csv', encoding='latin-1')\n",
    "df = df[['v1', 'v2']]\n",
    "df.columns = cols\n",
    "df['label']= df['label'].map({'ham': 0, 'spam': 1})\n",
    "df.head()\n",
    "print(len(df[df['label'] == 0]))\n",
    "print(len(df[df['label'] == 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75104853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>go until jurong point crazy available only in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>free entry in&nbsp;&nbsp;a wkly comp to win fa cup final...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>u dun say so early hor u c already then say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>nah i dont think he goes to usf he lives aroun...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      0  go until jurong point crazy available only in ...\n",
       "1      0                            ok lar joking wif u oni\n",
       "2      1  free entry in  a wkly comp to win fa cup final...\n",
       "3      0        u dun say so early hor u c already then say\n",
       "4      0  nah i dont think he goes to usf he lives aroun..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removing punctuation, numbers, and converting to lowercase\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "df['text'] = df['text'].apply(clean_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65f037c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a7b8bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x = X\n",
    "y = df['label']\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.25, random_state=42)\n",
    "\n",
    "metrics = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1af75299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97      1202\n",
      "           1       0.90      0.69      0.78       191\n",
      "\n",
      "    accuracy                           0.95      1393\n",
      "   macro avg       0.92      0.84      0.87      1393\n",
      "weighted avg       0.94      0.95      0.94      1393\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(max_depth=10, random_state=42, class_weight='balanced')\n",
    "# dt = DecisionTreeClassifier(max_depth=10, random_state=42)\n",
    "dt.fit(xtrain, ytrain)\n",
    "ypred = dt.predict(xtest)\n",
    "report = classification_report(ytest,ypred, output_dict=True)\n",
    "metrics.append(['Decision Tree', report['0']['precision'], report['1']['precision'], report['0']['recall'], report['1']['recall'], accuracy_score(ytest, ypred), report['weighted avg']['f1-score']])\n",
    "print(classification_report(ytest,ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34f33256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.98      1202\n",
      "           1       0.95      0.73      0.82       191\n",
      "\n",
      "    accuracy                           0.96      1393\n",
      "   macro avg       0.95      0.86      0.90      1393\n",
      "weighted avg       0.96      0.96      0.95      1393\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# rf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42, class_weight='balanced')\n",
    "rf.fit(xtrain, ytrain)\n",
    "ypred = rf.predict(xtest)\n",
    "report = classification_report(ytest,ypred, output_dict=True)\n",
    "metrics.append(['Random Forest', report['0']['precision'], report['1']['precision'], report['0']['recall'], report['1']['recall'], accuracy_score(ytest, ypred), report['weighted avg']['f1-score']])\n",
    "print(classification_report(ytest,ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1f87d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98      1202\n",
      "           1       0.99      0.79      0.87       191\n",
      "\n",
      "    accuracy                           0.97      1393\n",
      "   macro avg       0.98      0.89      0.93      1393\n",
      "weighted avg       0.97      0.97      0.97      1393\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "# svm = SVC()\n",
    "svm = SVC(class_weight='balanced')\n",
    "svm = svm.fit(xtrain, ytrain)\n",
    "ypred = svm.predict(xtest)\n",
    "report = classification_report(ytest,ypred, output_dict=True)\n",
    "metrics.append(['Support Vector Machine', report['0']['precision'], report['1']['precision'], report['0']['recall'], report['1']['recall'], accuracy_score(ytest, ypred), report['weighted avg']['f1-score']])\n",
    "print(classification_report(ytest,ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3e19008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98      1202\n",
      "           1       0.93      0.78      0.85       191\n",
      "\n",
      "    accuracy                           0.96      1393\n",
      "   macro avg       0.95      0.89      0.91      1393\n",
      "weighted avg       0.96      0.96      0.96      1393\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gbr = GradientBoostingClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "gbr.fit(xtrain, ytrain)\n",
    "ypred = gbr.predict(xtest)\n",
    "report = classification_report(ytest,ypred, output_dict=True)\n",
    "metrics.append(['Gradient Boosting', report['0']['precision'], report['1']['precision'], report['0']['recall'], report['1']['recall'], accuracy_score(ytest, ypred), report['weighted avg']['f1-score']])\n",
    "print(classification_report(ytest,ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f1b14b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      1.00      0.97      1202\n",
      "           1       0.99      0.62      0.77       191\n",
      "\n",
      "    accuracy                           0.95      1393\n",
      "   macro avg       0.97      0.81      0.87      1393\n",
      "weighted avg       0.95      0.95      0.94      1393\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(xtrain, ytrain)\n",
    "ypred = knn.predict(xtest)\n",
    "report = classification_report(ytest,ypred, output_dict=True)\n",
    "metrics.append(['K-Nearest Neighbors', report['0']['precision'], report['1']['precision'], report['0']['recall'], report['1']['recall'], accuracy_score(ytest, ypred), report['weighted avg']['f1-score']])\n",
    "print(classification_report(ytest,ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1bfa40d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98      1202\n",
      "           1       1.00      0.72      0.84       191\n",
      "\n",
      "    accuracy                           0.96      1393\n",
      "   macro avg       0.98      0.86      0.91      1393\n",
      "weighted avg       0.96      0.96      0.96      1393\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "nb = nb.fit(xtrain, ytrain)\n",
    "ypred = nb.predict(xtest)\n",
    "report = classification_report(ytest,ypred, output_dict=True)\n",
    "metrics.append(['Naive Bayes', report['0']['precision'], report['1']['precision'], report['0']['recall'], report['1']['recall'], accuracy_score(ytest, ypred), report['weighted avg']['f1-score']])\n",
    "print(classification_report(ytest,ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "978406a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98      1202\n",
      "           1       0.94      0.85      0.89       191\n",
      "\n",
      "    accuracy                           0.97      1393\n",
      "   macro avg       0.96      0.92      0.94      1393\n",
      "weighted avg       0.97      0.97      0.97      1393\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# lgr = LogisticRegression()\n",
    "lgr = LogisticRegression(class_weight='balanced')\n",
    "lgr= lgr.fit(xtrain, ytrain)\n",
    "ypred = lgr.predict(xtest)\n",
    "report = classification_report(ytest,ypred, output_dict=True)\n",
    "metrics.append(['Logistic Regression', report['0']['precision'], report['1']['precision'], report['0']['recall'], report['1']['recall'], accuracy_score(ytest, ypred), report['weighted avg']['f1-score']])\n",
    "print(classification_report(ytest,ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e27b927b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Precision(0)</th>\n",
       "      <th>Precision(1)</th>\n",
       "      <th>Recall(0)</th>\n",
       "      <th>Recall(1)</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.977</td>\n",
       "      <td>0.937</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.853</td>\n",
       "      <td>0.972</td>\n",
       "      <td>0.971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Support Vector Machine</td>\n",
       "      <td>0.967</td>\n",
       "      <td>0.987</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.785</td>\n",
       "      <td>0.969</td>\n",
       "      <td>0.968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>0.966</td>\n",
       "      <td>0.931</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.780</td>\n",
       "      <td>0.962</td>\n",
       "      <td>0.961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.957</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.717</td>\n",
       "      <td>0.961</td>\n",
       "      <td>0.958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.958</td>\n",
       "      <td>0.946</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.728</td>\n",
       "      <td>0.957</td>\n",
       "      <td>0.955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.952</td>\n",
       "      <td>0.897</td>\n",
       "      <td>0.988</td>\n",
       "      <td>0.686</td>\n",
       "      <td>0.946</td>\n",
       "      <td>0.943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>K-Nearest Neighbors</td>\n",
       "      <td>0.943</td>\n",
       "      <td>0.992</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.623</td>\n",
       "      <td>0.948</td>\n",
       "      <td>0.942</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Model  Precision(0)  Precision(1)  Recall(0)  Recall(1)  \\\n",
       "1     Logistic Regression         0.977         0.937      0.991      0.853   \n",
       "2  Support Vector Machine         0.967         0.987      0.998      0.785   \n",
       "3       Gradient Boosting         0.966         0.931      0.991      0.780   \n",
       "4             Naive Bayes         0.957         1.000      1.000      0.717   \n",
       "5           Random Forest         0.958         0.946      0.993      0.728   \n",
       "6           Decision Tree         0.952         0.897      0.988      0.686   \n",
       "7     K-Nearest Neighbors         0.943         0.992      0.999      0.623   \n",
       "\n",
       "   Accuracy  F1 Score  \n",
       "1     0.972     0.971  \n",
       "2     0.969     0.968  \n",
       "3     0.962     0.961  \n",
       "4     0.961     0.958  \n",
       "5     0.957     0.955  \n",
       "6     0.946     0.943  \n",
       "7     0.948     0.942  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_df  = pd.DataFrame(metrics, columns= ['Model', 'Precision(0)', 'Precision(1)', 'Recall(0)', 'Recall(1)', 'Accuracy', 'F1 Score'], index = [1,2,3,4,5,6,7]).drop_duplicates()\n",
    "for col in metrics_df.columns[1:]:\n",
    "    metrics_df[col] = metrics_df[col].apply(lambda x: round(x, 3))\n",
    "\n",
    "metrics_df.sort_values(by='F1 Score', ascending=False, inplace=True)\n",
    "metrics_df.reset_index(drop=True, inplace=True)\n",
    "metrics_df.index += 1\n",
    "metrics_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
